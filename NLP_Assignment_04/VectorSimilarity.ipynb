{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "powered-slide",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbassignment": {
     "type": "header"
    },
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "554bdab684eb16abce3c236bea52c459",
     "grade": false,
     "grade_id": "template_886979f3_0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    04\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Monday at 15:00</b>.</p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to three people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controversial-biography",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T07:53:51.698179Z",
     "start_time": "2025-04-30T07:53:51.682794Z"
    },
    "nbassignment": {
     "type": "group_info"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the username of each team member into the variables. \n",
    "If you work alone please leave the other variables empty.\n",
    "'''\n",
    "member1 = 'tghane2s'\n",
    "member2 = 'rmore2s'\n",
    "member3 = 'psheth2s'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-relationship",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8df404bcbde65df55117c4750706470f",
     "grade": false,
     "grade_id": "VectorSimilarity_AVectorSimilarity_BVectorSimilarity_CVectorSimilarity_DVectorSimilarity_EVectorSimilarity_FVectorSimilarity_G_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Word2Vec and FastText Embeddings\n",
    "\n",
    "In this assignment we will work on Word2Vec embeddings and FastText embeddings.\n",
    "\n",
    "I prepared three dictionaries for you:\n",
    "\n",
    "- ```word2vec_yelp_vectors.pkl```: A dictionary with 300 dimensional word2vec embeddings trained on the Google News Corpus, contains only words that are present in our Yelp reviews (key is the word, value is the embedding)\n",
    "- ```fasttext_yelp_vectors.pkl```: A dictionary with 300 dimensional FastText embeddings trained on the English version of Wikipedia, contains only words that are present in our Yelp reviews (key is the word, value is the embedding)\n",
    "- ```tfidf_yelp_vectors.pkl```: A dictionary with 400 dimensional TfIdf embeddings trained on the Yelp training dataset from last assignment (key is the word, value is the embedding)\n",
    "\n",
    "In the next cell we load those into the dictionaries ```w2v_vectors```, ```ft_vectors``` and ```tfidf_vectors```.\n",
    "\n",
    "Â© Tim Metzler, Hochschule Bonn-Rhein-Sieg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sensitive-following",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T07:53:52.127207Z",
     "start_time": "2025-04-30T07:53:51.704732Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc1c5e46315e7059176738283b3ff597",
     "grade": false,
     "grade_id": "VectorSimilarity_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/srv/shares/NLP/embeddings/word2vec_yelp_vectors.pkl', 'rb') as f:\n",
    "    w2v_vectors = pickle.loads(f.read())\n",
    "    \n",
    "with open('/srv/shares/NLP/embeddings/fasttext_yelp_vectors.pkl', 'rb') as f:\n",
    "    ft_vectors = pickle.loads(f.read())\n",
    "    \n",
    "with open('/srv/shares/NLP/embeddings/tfidf_yelp_vectors.pkl', 'rb') as f:\n",
    "    tfidf_vectors = pickle.loads(f.read())\n",
    "    \n",
    "with open('/srv/shares/NLP/datasets/yelp/reviews_train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "with open('/srv/shares/NLP/datasets/yelp/reviews_test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "reviews = train + test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-vermont",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff8b46bb90022c6288e48aee8a4cf1b7",
     "grade": false,
     "grade_id": "VectorSimilarity_A_Description1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Creating a vector model with helper functions [30 points]\n",
    "\n",
    "In the next cell we have the class ```VectorModel``` with the methods:\n",
    "\n",
    "- ```vector_size```: Returns the vector size of the model\n",
    "- ```embed```: Returns the embedding for a word. Returns None if there is no embedding present for the word\n",
    "- ```cosine_similarity```: Calculates the cosine similarity between two vectors\n",
    "- ```most_similar```: Given a word returns the ```top_n``` most similar words from the model, together with the similarity value, **sorted by similarity (descending)**. We do not want to return the word itself as the most similar one. So we only return the most similar words except for the first one.\n",
    "- ```most_similar_vec```: Given a vector returns the ```top_n``` most similar words from the model, together with the similarity value, **sorted by similarity (descending)**. Here we want to keep the most similar one.\n",
    "\n",
    "Your task is to complete these methods.\n",
    "\n",
    "Example output:\n",
    "```\n",
    "model = VectorModel(w2v_vectors)\n",
    "\n",
    "vector_good = model.embed('good')\n",
    "vector_tomato = model.embed('tomato')\n",
    "\n",
    "print(model.cosine_similarity(vector_good, vector_tomato)) # Prints: 0.05318105\n",
    "\n",
    "print(model.most_similar('tomato')) \n",
    "'''\n",
    "[('tomatoes', 0.8442263), \n",
    " ('lettuce', 0.70699364),\n",
    " ('strawberry', 0.6888598), \n",
    " ('strawberries', 0.68325955), \n",
    " ('potato', 0.67841727)]\n",
    "'''\n",
    "\n",
    "print(model.most_similar_vec(vector_good)) \n",
    "'''\n",
    "[('good', 1.0), \n",
    " ('great', 0.72915095), \n",
    " ('bad', 0.7190051), \n",
    " ('decent', 0.6837349), \n",
    " ('nice', 0.68360925)]\n",
    "'''\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "honest-little",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T07:53:52.294079Z",
     "start_time": "2025-04-30T07:53:52.131792Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8dcf13a0fc232d23f818a43ce16c475",
     "grade": false,
     "grade_id": "VectorSimilarity_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05318105\n",
      "[('tomatoes', 0.8442263), ('lettuce', 0.70699376), ('strawberry', 0.6888598), ('strawberries', 0.6832595), ('potato', 0.67841715)]\n",
      "[('good', 1.0), ('great', 0.72915095), ('bad', 0.7190051), ('decent', 0.6837348), ('nice', 0.68360925)]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "   \n",
    "class VectorModel:\n",
    "    \n",
    "    def __init__(self, vector_dict: Dict[str, np.ndarray]):\n",
    "        # YOUR CODE HERE\n",
    "        self.vector = vector_dict\n",
    "        \n",
    "    def embed(self, word: str) -> np.ndarray:\n",
    "        # YOUR CODE HERE\n",
    "        return self.vector.get(word, None)\n",
    "    \n",
    "    def vector_size(self) -> int:\n",
    "        # YOUR CODE HERE\n",
    "        return len(next(iter(self.vector.values())))\n",
    "    \n",
    "    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        dot = np.dot(vec1, vec2)\n",
    "        norm = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "        return dot / norm if norm != 0 else 0.0\n",
    "\n",
    "    def most_similar(self, word: str, top_n: int=5) -> List[Tuple[str, float]]:\n",
    "        target_vec = self.embed(word)\n",
    "        if target_vec is None:\n",
    "            return []\n",
    "        \n",
    "        similarities = []\n",
    "        for other_word, vec in self.vector.items():\n",
    "            if other_word == word:\n",
    "                continue\n",
    "            similarity = self.cosine_similarity(target_vec, vec)\n",
    "            similarities.append((other_word, similarity))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "        \n",
    "    def most_similar_vec(self, vec: np.ndarray, top_n: int=5) -> List[Tuple[str, float]]:\n",
    "        similarities = []\n",
    "        for word, word_vec in self.vector.items():\n",
    "            similarity = self.cosine_similarity(vec, word_vec)\n",
    "            similarities.append((word, similarity))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "model = VectorModel(w2v_vectors)\n",
    "vector_good = model.embed('good')\n",
    "vector_tomato = model.embed('tomato')\n",
    "\n",
    "print(model.cosine_similarity(vector_good, vector_tomato))\n",
    "print(model.most_similar('tomato'))\n",
    "print(model.most_similar_vec(vector_good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-fault",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:33:22.664079Z",
     "start_time": "2025-04-28T12:33:21.338934Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a275be8f0df6b5af65a5ce0667dd603f",
     "grade": true,
     "grade_id": "test_VectorSimilarity_A0",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wicked-norway",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc8f7068215ca1ef592778c5a4b4162a",
     "grade": false,
     "grade_id": "VectorSimilarity_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity A) [10 points]\n",
    "\n",
    "We now want to find the most similar words for a given input word for each model (Word2Vec, FastText and TfIdf).\n",
    "\n",
    "Your input words are: ```['good', 'tomato', 'restaurant', 'beer', 'wonderful']```.\n",
    "\n",
    "For each model and input word print the top three most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "improved-count",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T07:53:53.451020Z",
     "start_time": "2025-04-30T07:53:52.300097Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddad901f2d646951490e9fcfe8edb004",
     "grade": true,
     "grade_id": "VectorSimilarity_B",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Word2Vec\n",
      "good: [('great', 0.72915095), ('bad', 0.7190051), ('decent', 0.6837348)]\n",
      "tomato: [('tomatoes', 0.8442263), ('lettuce', 0.70699376), ('strawberry', 0.6888598)]\n",
      "restaurant: [('restaurants', 0.77228934), ('diner', 0.72802156), ('steakhouse', 0.72698534)]\n",
      "beer: [('beers', 0.8409688), ('drinks', 0.66893125), ('ale', 0.63828725)]\n",
      "wonderful: [('fantastic', 0.8047919), ('great', 0.76478696), ('fabulous', 0.7614761)]\n",
      "dinner: [('dinners', 0.7902064), ('brunch', 0.79005134), ('breakfast', 0.7007028)]\n",
      "\n",
      "Model: FastText\n",
      "good: [('excellent', 0.7223856825801254), ('decent', 0.7202461451724537), ('bad', 0.6704173041669614)]\n",
      "tomato: [('eggplant', 0.7518509618329048), ('spinach', 0.7422800959168396), ('onions', 0.7328857483500281)]\n",
      "restaurant: [('restaurants', 0.8384667264823358), ('bistro', 0.7845601578005464), ('bakery', 0.7155727705943096)]\n",
      "beer: [('beers', 0.7944971406865431), ('brewed', 0.7929903321082489), ('brewery', 0.7520785637582763)]\n",
      "wonderful: [('lovely', 0.6808215868395576), ('fascinating', 0.6745727685452472), ('amazing', 0.6457084279396067)]\n",
      "dinner: [('dinners', 0.8463012295986414), ('brunch', 0.709906334988423), ('meal', 0.6719537670739056)]\n",
      "\n",
      "Model: TfIdf\n",
      "good: [('the', 0.6199071144484399), ('a', 0.6170194328254505), ('and', 0.6121212998064655)]\n",
      "tomato: [('provolone', 0.7071067811865476), ('ceasar', 0.7071067811865475), ('cheesesteak', 0.7071067811865475)]\n",
      "restaurant: [('held', 0.46881558553353003), ('patrons', 0.46178340768084236), ('we', 0.4308073684733683)]\n",
      "beer: [('tap', 0.6326077388711026), ('beers', 0.5132564299212761), ('blowingly', 0.4746774221449982)]\n",
      "wonderful: [('truffle', 0.6264995084522798), ('accident', 0.5432509277196604), ('equally', 0.5432509277196604)]\n",
      "dinner: [('slaw', 0.4842078303350308), ('timely', 0.4038564890706335), ('plates', 0.3828945646253459)]\n"
     ]
    }
   ],
   "source": [
    "input_words = ['good', 'tomato', 'restaurant', 'beer', 'wonderful', 'dinner']\n",
    "\n",
    "w2v_vm = VectorModel(w2v_vectors)          # Word2Vec\n",
    "fasttext_vm = VectorModel(ft_vectors) # FastText\n",
    "tfidf_vm = VectorModel(tfidf_vectors)       # TF-IDF vectors \n",
    "models = {\n",
    "    'Word2Vec': w2v_vm,\n",
    "    'FastText': fasttext_vm,\n",
    "    'TfIdf': tfidf_vm\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    for word in input_words:\n",
    "        if model.embed(word) is not None:\n",
    "            top_similar = model.most_similar(word, top_n=3)\n",
    "            print(f\"{word}: {top_similar}\")\n",
    "        else:\n",
    "            print(f\"{word}: Not found in vocabulary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-ultimate",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea42db99d9e4f8eceab5247a1e742d2f",
     "grade": false,
     "grade_id": "VectorSimilarity_C_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity B) [10 points]\n",
    "\n",
    "Comment on the output from the previous task. Let us look at the output for the word ```wonderful```. How do the models differ for this word? Can you reason why the TfIdf model shows so different results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-aviation",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "560f030fbc0ebf9c093d42f0af3d37c5",
     "grade": true,
     "grade_id": "VectorSimilarity_C",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "# - Word2Vec\n",
    "\n",
    "### wonderful: [('fantastic', 0.8047919), ('great', 0.76478696), ('fabulous', 0.7614761)]\n",
    "These are clear synonyms or near-synonyms of \"wonderful\". This shows that Word2Vec captures semantic relationships very well, since it learns from context; if words appear in similar contexts, they get similar vectors.\n",
    "\n",
    "# - FastText\n",
    "\n",
    "### wonderful: [('lovely', 0.6808215868395576), ('fascinating', 0.6745727685452472), ('amazing', 0.6457084279396067)]\n",
    "FastText works similarly to Word2Vec but also includes subword information (n-grams), which helps with rare or misspelled words.\n",
    "\n",
    "# - TfIdf\n",
    "\n",
    "### wonderful: [('truffle', 0.6264995084522798), ('accident', 0.5432509277196604), ('equally', 0.5432509277196604)]\n",
    "These words are not semantically similar to \"wonderful\". It doesn't understand meaning, only surface-level correlations.\n",
    "\n",
    "TfIdf gives dissimilar results for \"wonderful\" because it lacks understanding of semantic similarity. Word2Vec and FastText are trained on massive corpora using context windows, which enables them to reflect true meaning-based similarity, making them more suitable for tasks requiring semantic understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-answer",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0577bd13c5771b100b73b4d013022259",
     "grade": false,
     "grade_id": "VectorSimilarity_D_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity C) [10 points]\n",
    "\n",
    "Instead of just finding the most similar word to a single word, we can also find the most similar word given a list of positive and negative words.\n",
    "\n",
    "For this we just sum up the positive and negative words into a single vector by calculating a weighted mean. For this we multiply each positive word with a factor of $+1$ and each negative word with a factor of $-1$. Then we get the most similar words to that vector.\n",
    "\n",
    "You are given the following examples:\n",
    "\n",
    "```\n",
    "inputs = [\n",
    "    {\n",
    "        'positive': ['good', 'wonderful'],\n",
    "        'negative': ['bad']\n",
    "    },\n",
    "    {\n",
    "        'positive': ['tomato', 'lettuce'],\n",
    "        'negative': ['strawberry', 'salad']\n",
    "    }    \n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advanced-shelf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T07:53:53.667163Z",
     "start_time": "2025-04-30T07:53:53.458454Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c11d7cc8e7d41bae0b33df45bc3e5433",
     "grade": true,
     "grade_id": "VectorSimilarity_D",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input 1: {'positive': ['good', 'wonderful'], 'negative': ['bad']}\n",
      "Top match: [('wonderful', 0.5333514)]\n",
      "\n",
      "Input 2: {'positive': ['tomato', 'lettuce'], 'negative': ['strawberry', 'fruit']}\n",
      "Top match: [('lettuce', 0.54421157)]\n",
      " Warning: Positive word 'ceasar' not in vocabulary.\n",
      "\n",
      "Input 3: {'positive': ['ceasar', 'chicken'], 'negative': []}\n",
      "Top match: [('chicken', 0.9999999977182473)]\n"
     ]
    }
   ],
   "source": [
    "# Answer\n",
    "\n",
    "def find_analogy_vector(model_vectors: Dict[str, np.ndarray], word_groups: Dict[str, List[str]]) -> List[Tuple[str, float]]:\n",
    "    model = VectorModel(model_vectors)\n",
    "\n",
    "    # Lists to store valid vectors\n",
    "    pos_vectors = []\n",
    "    neg_vectors = []\n",
    "\n",
    "    # Collect positive vectors\n",
    "    for pos_word in word_groups.get(\"positive\", []):\n",
    "        vec = model.embed(pos_word)\n",
    "        if vec is not None:\n",
    "            pos_vectors.append(vec)\n",
    "        else:\n",
    "            print(f\" Warning: Positive word '{pos_word}' not in vocabulary.\")\n",
    "\n",
    "    # Collect negative vectors\n",
    "    for neg_word in word_groups.get(\"negative\", []):\n",
    "        vec = model.embed(neg_word)\n",
    "        if vec is not None:\n",
    "            neg_vectors.append(vec)\n",
    "        else:\n",
    "            print(f\" Warning: Negative word '{neg_word}' not in vocabulary.\")\n",
    "\n",
    "    # Compute the average positive and negative vectors\n",
    "    pos_mean = np.mean(pos_vectors, axis=0) if pos_vectors else np.zeros(model.vector_size())\n",
    "    neg_mean = np.mean(neg_vectors, axis=0) if neg_vectors else np.zeros(model.vector_size())\n",
    "\n",
    "    # Final analogy vector\n",
    "    analogy_vec = pos_mean - neg_mean\n",
    "\n",
    "    # Return top 1 most similar word to the resulting vector\n",
    "    return model.most_similar_vec(analogy_vec, top_n=1)\n",
    "\n",
    "inputs = [\n",
    "    {\n",
    "        'positive': ['good', 'wonderful'],\n",
    "        'negative': ['bad']\n",
    "    },\n",
    "    {\n",
    "        'positive': ['tomato', 'lettuce'],\n",
    "        'negative': ['strawberry', 'fruit']\n",
    "    },\n",
    "    {\n",
    "        'positive': ['ceasar', 'chicken'],\n",
    "        'negative': []\n",
    "    }    \n",
    "]\n",
    "for idx, entry in enumerate(inputs):\n",
    "    result = find_analogy_vector(w2v_vectors, entry)\n",
    "    print(f\"\\nInput {idx + 1}: {entry}\")\n",
    "    print(f\"Top match: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-reset",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3aeb5076326c30f2b229061cc1d1dc58",
     "grade": false,
     "grade_id": "VectorSimilarity_E_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Investigating similarity D) [15 points]\n",
    "\n",
    "We can use our model to find out which word does not match given a list of words.\n",
    "\n",
    "For this we build the mean vector of all embeddings in the list.  \n",
    "Then we calculate the cosine similarity between the mean and all those vectors.\n",
    "\n",
    "The word that does not match is then the word with the lowest cosine similarity to the mean.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "model = VectorModel(w2v_vectors)\n",
    "doesnt_match(model, ['potato', 'tomato', 'beer']) # -> 'beer'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "featured-ballot",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T07:53:53.690176Z",
     "start_time": "2025-04-30T07:53:53.672706Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07a2a155084a8433ed5f6a649b4b9c3b",
     "grade": false,
     "grade_id": "VectorSimilarity_E",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORDS: ['vegetable', 'strawberry', 'tomato', 'lettuce'], \n",
      "Doesn't match: vegetable\n",
      "\n",
      "WORDS: ['potato', 'tomato', 'beer'], \n",
      "Doesn't match: beer\n"
     ]
    }
   ],
   "source": [
    "def doesnt_match(model, word_list):\n",
    "    # List to store cosine similarity scores\n",
    "    sim_scores = []\n",
    "\n",
    "    # Vector to hold the sum of embeddings\n",
    "    combined_vec = np.zeros(model.vector_size())\n",
    "\n",
    "    # Sum up all the word embeddings\n",
    "    for token in word_list:\n",
    "        vec = model.embed(token)\n",
    "        combined_vec += vec\n",
    "\n",
    "    # Calculate the average (mean) embedding\n",
    "    avg_vec = combined_vec / len(word_list)\n",
    "\n",
    "    # Compute cosine similarity of each word vector with the mean vector\n",
    "    for token in word_list:\n",
    "        vec = model.embed(token)\n",
    "        sim = model.cosine_similarity(vec, avg_vec)\n",
    "        sim_scores.append(sim)\n",
    "\n",
    "    # Convert similarity list to numpy array and find index of lowest score\n",
    "    sim_scores = np.array(sim_scores)\n",
    "    least_similar_idx = np.argmin(sim_scores)\n",
    "\n",
    "    # Return the word least similar to the group\n",
    "    return word_list[least_similar_idx]\n",
    "\n",
    "\n",
    "doesnt_match(VectorModel(w2v_vectors), ['vegetable', 'strawberry', 'tomato', 'lettuce'])\n",
    "words_1 = ['vegetable', 'strawberry', 'tomato', 'lettuce']\n",
    "odd_word = doesnt_match(VectorModel(w2v_vectors), words_1)\n",
    "print(f\"\\nWORDS: {words_1}, \\nDoesn't match: {odd_word}\")\n",
    "\n",
    "words_2 = ['potato', 'tomato', 'beer']\n",
    "odd_word = doesnt_match(VectorModel(w2v_vectors), words_2)\n",
    "print(f\"\\nWORDS: {words_2}, \\nDoesn't match: {odd_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-conclusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:33:32.562401Z",
     "start_time": "2025-04-28T12:33:32.422819Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b0ba5c75465a8e985326f69ef7da279",
     "grade": true,
     "grade_id": "test_VectorSimilarity_E0",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "responsible-parent",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d141ef5c3f341644d3a756404c50be53",
     "grade": false,
     "grade_id": "VectorSimilarity_F_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Document Embeddings A) [15 points]\n",
    "\n",
    "Now we want to create document embeddings similar to the last assignment. For this you are given the function ```bagOfWords```. In the context of Word2Vec and FastText embeddings this is also called ```SOWE``` for sum of word embeddings.\n",
    "\n",
    "Take the yelp reviews (```reviews```) and create a dictionary containing the document id as a key and the document embedding as a value.\n",
    "\n",
    "Create the document embeddings from the Word2Vec, FastText and TfIdf embeddings.\n",
    "\n",
    "Store these in the variables ```ft_doc_embeddings```, ```w2v_doc_embeddings``` and ```tfidf_doc_embeddings```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "serial-forum",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T07:53:54.126504Z",
     "start_time": "2025-04-30T07:53:53.697479Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e69f73310a129b0adbface4154bf02c5",
     "grade": false,
     "grade_id": "VectorSimilarity_F",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def bagOfWords(model, doc: List[str]) -> np.ndarray:\n",
    "    '''\n",
    "    Create a document embedding using the bag of words approach\n",
    "    \n",
    "    Args:\n",
    "        model     -- The embedding model to use\n",
    "        doc       -- A document as a list of tokens\n",
    "        \n",
    "    Returns:\n",
    "        embedding -- The embedding for the document as a single vector \n",
    "    '''\n",
    "    embeddings = [np.zeros(model.vector_size())]\n",
    "    n_tokens = 0\n",
    "    for token in doc:\n",
    "        embedding = model.embed(token)\n",
    "        if embedding is not None:\n",
    "            n_tokens += 1\n",
    "            embeddings.append(embedding)\n",
    "    if n_tokens > 0:\n",
    "        return sum(embeddings)/n_tokens\n",
    "    return sum(embeddings)\n",
    "\n",
    "ft_doc_embeddings = dict()\n",
    "w2v_doc_embeddings = dict()\n",
    "tfidf_doc_embeddings = dict()\n",
    "\n",
    "# document-level embeddings\n",
    "for review in reviews:\n",
    "    doc_id = review['id']\n",
    "    tokens = review['tokens']\n",
    "    \n",
    "    w2v_doc_embeddings[doc_id] = bagOfWords(w2v_vm, tokens)\n",
    "    ft_doc_embeddings[doc_id] = bagOfWords(fasttext_vm, tokens)\n",
    "    tfidf_doc_embeddings[doc_id] = bagOfWords(tfidf_vm, tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-forwarding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T12:33:35.204692Z",
     "start_time": "2025-04-28T12:33:34.966597Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c1d7d5c9497b1ebaf9a44d10aadb1a4",
     "grade": true,
     "grade_id": "test_VectorSimilarity_F0",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "advisory-slovak",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ea86310ebff375544ce2e08fff4cff9",
     "grade": false,
     "grade_id": "VectorSimilarity_G_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Document Embeddings B) [10 points]\n",
    "\n",
    "Create a vector model from each of the document embedding dictionaries. Call these ```model_w2v_doc```, ```model_ft_doc``` and ```model_tfidf_doc```.\n",
    "\n",
    "Now find the most similar document (```top_n=1```) for document $438$ with each of these models. Use the method `most_similar`. For example `model.most_similar(438)`.\n",
    "\n",
    "Print the text for each of the most similar reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "persistent-fairy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T07:55:24.354123Z",
     "start_time": "2025-04-30T07:55:24.314295Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71ccd4569a5255386cafbbbc1cc0b510",
     "grade": true,
     "grade_id": "VectorSimilarity_G",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source document:\n",
      "Absolutely ridiculously amazing! Chicken Tikka masala was perfect. Best I've ever had!\n",
      "\n",
      "Most similar document (Word2Vec):\n",
      "I LOVE THIS PLACE. ever since i frist tried it in atl.\n",
      "\n",
      "ok. first thing first. YOU MUST ORDER COCONUT SHRIMP (unless your allergic!)\n",
      "\n",
      "we ordered:\n",
      "\n",
      "onion rings ( A MUST its frikkin huge and the sauce is awesome!)\n",
      "\n",
      "Crab, Shrimp, Mango and Avocado Stack Crab (its frikkin good as hell.. mix it in the sauce that is circled around the plate they go perfect together!)\n",
      "\n",
      "Cuban Sandwich (eggs were missing from it for me but the bf said it was good)\n",
      "\n",
      "1/2 coconut shrimp & Caesar salad. (like I said the coconut shrimp is to die for ::and if your allergic i mean literally::  the caesar salad was... meh.. the cruton were cute but the salad itself was whatevers.\n",
      "\n",
      "SERVICE AS USUAL WAS AWESOME! waiters were nice, patient and quick. waters were always filled as well.\n",
      "\n",
      "Most similar document (FastText):\n",
      "Been there three times, and have tried out different dishes.\n",
      "\n",
      "First time i got pad thai, omg, that food was sooooooooooo greasy and salty, but i swore that i was not going back there again. \n",
      "\n",
      "But my coworker wants to go there for lunch and said the curry was good, so i went back again. This time i had the green curry with beef, it was pretty decent, flavorful yet not too strong. however, the veggie inside is a little bit more chewy that i would have prefer.\n",
      "\n",
      "Then we went back again for lunch, this time i had the noodles, ca'n't remember what it is called. it is those very thin and transparent one. It was really good. i ordered 4/5 spiciness, but still not too spicy. \n",
      "\n",
      "So basically it might be a hit or a miss, and it is not super spicy so if you are looking for that you might be disappointed;. overall a decent lunch spot.\n",
      "\n",
      "Most similar document (TF-IDF):\n",
      "Food is reasonably cheap and tasty. Especially their spicy chicken and jerk chicken. Perfect when you want to grab something filling on the go for less than $10.\n",
      "\n",
      "The staff there could benefit from being less overly stingy...the staff took almost 1 full minute picking a piece of chicken for me to sample (after she offered nonetheless) and would keep skewering pieces that were \"too large\" apparently, take it off the toothpick and try again. Even when filling the containers, the staff are very very careful to not put too much food, counting out exact pieces of chicken from the curries and even taking some out of the container after.\n"
     ]
    }
   ],
   "source": [
    "# First find the text for review 438\n",
    "def find_doc(doc_id, reviews):\n",
    "    for review in reviews:\n",
    "        if review['id'] == doc_id:\n",
    "            return review['text']\n",
    "    \n",
    "doc_id = 438\n",
    "\n",
    "# Print it\n",
    "print('Source document:')\n",
    "print(find_doc(doc_id, reviews))\n",
    "\n",
    "# Create the models\n",
    "model_w2v_doc = VectorModel(w2v_doc_embeddings)\n",
    "model_ft_doc = VectorModel(ft_doc_embeddings)\n",
    "model_tfidf_doc = VectorModel(tfidf_doc_embeddings)\n",
    "\n",
    "# similar documents\n",
    "similar_doc_w2v = model_w2v_doc.most_similar(doc_id, top_n=2)[1][0]\n",
    "similar_doc_ft = model_ft_doc.most_similar(doc_id, top_n=2)[1][0]\n",
    "similar_doc_tfidf = model_tfidf_doc.most_similar(doc_id, top_n=2)[1][0]\n",
    "\n",
    "# print\n",
    "print(\"\\nMost similar document (Word2Vec):\")\n",
    "print(find_doc(similar_doc_w2v, reviews))\n",
    "\n",
    "print(\"\\nMost similar document (FastText):\")\n",
    "print(find_doc(similar_doc_ft, reviews))\n",
    "\n",
    "print(\"\\nMost similar document (TF-IDF):\")\n",
    "print(find_doc(similar_doc_tfidf, reviews))\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
