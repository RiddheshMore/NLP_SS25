{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "powered-slide",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbassignment": {
     "type": "header"
    },
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7503215340b20c8cc44b53567157319d",
     "grade": false,
     "grade_id": "template_886979f3_0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Natural Language Processing</h1>\n",
    "    <h3>General Information:</h3>\n",
    "    <p>Please do not add or delete any cells. Answers belong into the corresponding cells (below the question). If a function is given (either as a signature or a full function), you should not change the name, arguments or return value of the function.<br><br> If you encounter empty cells underneath the answer that can not be edited, please ignore them, they are for testing purposes.<br><br>When editing an assignment there can be the case that there are variables in the kernel. To make sure your assignment works, please restart the kernel and run all cells before submitting (e.g. via <i>Kernel -> Restart & Run All</i>).</p>\n",
    "    <p>Code cells where you are supposed to give your answer often include the line  ```raise NotImplementedError```. This makes it easier to automatically grade answers. If you edit the cell please outcomment or delete this line.</p>\n",
    "    <h3>Submission:</h3>\n",
    "    <p>Please submit your notebook via the web interface (in the main view -> Assignments -> Submit). The assignments are due on <b>Monday at 15:00</b></p>\n",
    "    <h3>Group Work:</h3>\n",
    "    <p>You are allowed to work in groups of up to three people. Please enter the UID (your username here) of each member of the group into the next cell. We apply plagiarism checking, so do not submit solutions from other people except your team members. If an assignment has a copied solution, the task will be graded with 0 points for all people with the same solution.</p>\n",
    "    <h3>Questions about the Assignment:</h3>\n",
    "    <p>If you have questions about the assignment please post them in the LEA forum before the deadline. Don't wait until the last day to post questions.</p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controversial-biography",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T11:31:23.579705Z",
     "start_time": "2025-04-22T11:31:23.561541Z"
    },
    "nbassignment": {
     "type": "group_info"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Group Work:\n",
    "Enter the UID of each team member into the variables. \n",
    "If you work alone please leave the other variables empty.\n",
    "'''\n",
    "member1 = 'tghane2s'\n",
    "member2 = 'rmore2s'\n",
    "member3 = 'psheth2s'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-spanish",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49ee22d6c2e3e271dc00016f477e349b",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_ATfIdfEmbeddings_BTfIdfEmbeddings_CTfIdfEmbeddings_DTfIdfEmbeddings_ETfIdfEmbeddings_FTfIdfEmbeddings_GTfIdfEmbeddings_HTfIdfEmbeddings_I_Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Word Embeddings\n",
    "\n",
    "In this assignment we want to create word embeddings (vectors) from a corpus and use them to build a simple document classifier.\n",
    "\n",
    "For this we will use a subset of the Yelp restaurant dataset. \n",
    "\n",
    "The dataset contains a list of reviews, where each review is a dictionary with the following fields:\n",
    "\n",
    "- ```id```: An integer denoting the id of the review\n",
    "- ```text```: The original review as a single string\n",
    "- ```stars```: How the place was rated from 1 (worst) to 5 (best)\n",
    "- ```tokens```: The tokenized and cleaned review. All tokens have been converted to lower case.\n",
    "\n",
    "There are two datasets, ```reviews_train``` consisting of 400 reviews (80 per rating) and ```reviews_test``` consisting of 100 reviews (20 per rating).\n",
    "\n",
    "The following cell shows how to load the datasets into the variables ```train``` and ```test```.\n",
    "\n",
    "\n",
    "**Attention: This assignment might look like a lot of tasks but you will be able to reuse a lot of answers from the One Hot Embedding tasks for the TfIdf Embedding task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lasting-alliance",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T08:40:01.970405Z",
     "start_time": "2025-04-22T08:40:01.905600Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0207c972397cf0819a148a91d2a36cc",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_A_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 438,\n",
       " 'text': \"Absolutely ridiculously amazing! Chicken Tikka masala was perfect. Best I've ever had!\",\n",
       " 'tokens': ['absolutely',\n",
       "  'ridiculously',\n",
       "  'amazing',\n",
       "  'chicken',\n",
       "  'tikka',\n",
       "  'masala',\n",
       "  'was',\n",
       "  'perfect',\n",
       "  'best',\n",
       "  'i',\n",
       "  'ever',\n",
       "  'had'],\n",
       " 'stars': 5}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/srv/shares/NLP/datasets/yelp/reviews_train.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "with open('/srv/shares/NLP/datasets/yelp/reviews_test.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "# Print a review\n",
    "train[358]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee719537",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T08:40:05.470868Z",
     "start_time": "2025-04-22T08:40:05.458831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 20, 2: 20, 3: 20, 4: 20, 5: 20})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([review[\"stars\"] for review in test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-scroll",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd47451ee604c64cc4a188e8cf6c8471",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_A_Description1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "Perhabs the simplest way to build word embeddings is by using one hot vectors. These are vectors with a length of $|V|$ where $V$ is our vocabulary (the types from our corpus). **The vocabulary should be an alphabetically sorted list of types.**\n",
    "\n",
    "A one hot vector consists of mostly zeros with a single one at the index of the word in the vocabulary. \n",
    "\n",
    "*Example:*\n",
    "\n",
    "Assume we have the vocabulary ```['a', 'dog', 'i', 'have']```. Then the embedding for the word ```a``` would be $[1, 0, 0, 0]$. The embedding for the word ```i``` would be $[0, 0, 1, 0]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-partner",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7cd2f745e28c19146b230860589c8556",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_A_Description2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### One Hot Encoding A) [15 points]\n",
    "\n",
    "Complete the class ```OneHotModel```.\n",
    "\n",
    "This class has the following three methods:\n",
    "\n",
    "- ```build_index```: Takes a list of documents (each being a list of tokens). Then it creates the dictionary ```self.index``` which maps each type to an index. In the example above the dictionary would look like this:\n",
    "```self.index = {'a': 0, 'dog': 1, 'i': 2, 'have': 3}``` **It should be sorted alphabetically!**\n",
    "\n",
    "- ```train```: Takes a list of documents (each being a list of tokens). This should train the one hot model such that it can return vectors for each type in the corpus\n",
    "\n",
    "- ```embed```: Take a single word and return a one hot encoded vector for that word. If the word is not in the index it should return ```None```\n",
    "\n",
    "\n",
    "Please complete the three methods of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-argument",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T11:34:15.650598Z",
     "start_time": "2024-06-03T11:34:15.448334Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72d04908520258288cd4602adbb8696f",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_A",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Abstract super class for all embedding models we build, do not edit this\n",
    "class EmbeddingModel(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, docs: List[List[str]]) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def embed(self, word: str) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def vector_size(self) -> int:\n",
    "        pass\n",
    "\n",
    "\n",
    "class OneHotModel(EmbeddingModel):\n",
    "    \n",
    "    def build_index(self, docs: List[List[str]]) -> None:\n",
    "        '''\n",
    "        Create an index for the vocabulary from the docs\n",
    "        The index should be alphabetically sorted!\n",
    "        \n",
    "        Args:\n",
    "            docs -- A list of documents where each document is a list of tokens\n",
    "        '''\n",
    "        self.index = dict()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def train(self, docs: List[List[str]]) -> None:\n",
    "        '''\n",
    "        Train our model with a list of documents\n",
    "        In the case of one hot encoding training is just building the index!\n",
    "        \n",
    "        Args:\n",
    "            docs -- A list of documents where each document is a list of tokens\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def embed(self, word: str) -> np.ndarray:\n",
    "        '''\n",
    "        Embed a word into our one hot vector space\n",
    "        If the word is not in the index it will return None\n",
    "        \n",
    "        Args:\n",
    "            word      -- The word we want an embedding for\n",
    "        Returns:\n",
    "            embedding -- The one hot encoded vector for the word\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def vector_size(self) -> int:\n",
    "        '''\n",
    "        Return the length of the embedding\n",
    "        '''\n",
    "        return len(self.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-gates",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T11:34:15.705965Z",
     "start_time": "2024-06-03T11:34:15.652810Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7006ecaf4bb8091256a1ef55f0af7308",
     "grade": true,
     "grade_id": "test_TfIdfEmbeddings_A0",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-parker",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e0527a2c04fe3be2b79518d938c2bef",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_B_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### One Hot Encoding B) [10 points]\n",
    "\n",
    "We now want to use our model to create one hot vectors for each type in our training dataset ```train```.\n",
    "\n",
    "After training we want to create document embeddings using the bag-of-words approach. For this you need to complete a function called ```bagOfWords``` which takes in a document as a list of strings and our one hot model, maps each token of the document to a one hot vector and sums them up. \n",
    "\n",
    "The document embedding is then the sum of all word embeddings in the document **divided by the number of tokens in the document that are <u>present</u> in our model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-boost",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T11:34:15.725769Z",
     "start_time": "2024-06-03T11:34:15.708437Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95e1b2cd948c605ba9bd4e788834466f",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_B",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def bagOfWords(model: EmbeddingModel, doc: List[str]) -> np.ndarray:\n",
    "    '''\n",
    "    Create a document embedding using the bag of words approach\n",
    "    \n",
    "    Args:\n",
    "        model     -- The embedding model to use\n",
    "        doc       -- A document as a list of tokens\n",
    "        \n",
    "    Returns:\n",
    "        embedding -- The embedding for the document as a single vector \n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "# Create a one hot model and train it on a dummy corpus\n",
    "model = OneHotModel()\n",
    "corpus = [['i', 'like', 'pizza'],\n",
    "          ['do', 'you', 'like', 'pizza'],\n",
    "          ['everybody', 'likes', 'pizza', 'or', 'fries']]\n",
    "\n",
    "# Train the model on the corpus\n",
    "model.train(corpus)\n",
    "\n",
    "# Create a document embedding for the sample document\n",
    "doc = ['you', 'like', 'many', 'fries', 'fries']\n",
    "\n",
    "# This should create the embedding: [0, 0, 0.5, 0, 0.25, 0, 0, 0, 0.25]\n",
    "bagOfWords(model, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-spectacular",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T11:34:15.793184Z",
     "start_time": "2024-06-03T11:34:15.729455Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c0ad0d9cd78f696bdbb2d468ec25b52",
     "grade": true,
     "grade_id": "test_TfIdfEmbeddings_B0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell, please ignore it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-thomas",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25cb82de93ba73031b729120cab3486d",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_C_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### One Hot Encoding C) [10 points]\n",
    "\n",
    "Train your OneHotModel on the reviews from the training set.\n",
    "\n",
    "Then create the following matrices / vectors from the training and test dataset:\n",
    "\n",
    "- ```embed_train```: A 2-dimensional numpy array where the rows are the document embeddings for each document in the training set\n",
    "- ```labels_train```: A 1-dimensional numpy array where each element is the rating (stars) of the review from the training set. The rating at position 3 should correspond to the third row of the ```embed_train``` matrix.\n",
    "- ```embed_test```: A 2-dimensional numpy array where the rows are the document embeddings for each document in the test set\n",
    "- ```labels_test```: A 1-dimensional numpy array where each element is the rating (stars) of the review from the test set. The rating at position 3 should correspond to the third row of the ```embed_test``` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-desire",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T11:34:16.646043Z",
     "start_time": "2024-06-03T11:34:15.796172Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d172922582c34ea4cd46f2882660de74",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_C",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = OneHotModel()\n",
    "model.train([review['tokens'] for review in train])\n",
    "\n",
    "embed_train = np.array([[]])\n",
    "labels_train = np.array([])\n",
    "\n",
    "embed_test = np.array([[]])\n",
    "labels_test = np.array([])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(embed_train.shape)  # Should print (400, 5497)\n",
    "print(labels_train.shape) # Should print (400, )\n",
    "print(embed_test.shape)   # Should print (100, 5497)\n",
    "print(labels_test.shape)  # Should print (100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-privilege",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T11:34:16.886033Z",
     "start_time": "2024-06-03T11:34:16.648134Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3728f73bcad5505c3fe42651ca6fb5c1",
     "grade": true,
     "grade_id": "test_TfIdfEmbeddings_C0",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc0ee7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T11:34:16.888513Z",
     "start_time": "2024-06-03T11:34:16.888498Z"
    }
   },
   "outputs": [],
   "source": [
    "print(embed_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-tourist",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ff7e3c28b743a0823e8f448f347e170",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_D_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### One Hot Encoding D) [10 points]\n",
    "\n",
    "With the matrices and vectors we just created we can train a simple classifier such as a Support Vector Machine for multi-class classification. \n",
    "\n",
    "For this we will use an implementation from sklearn ```sklearn.svm.SVC```. Each model from sklearn has two main methods:\n",
    "\n",
    "- ```fit```: Takes in a matrix like our ```embed_train``` matrix and a vector like our ```labels_train``` vector and trains the classifier on the data\n",
    "\n",
    "- ```predict```: Takes in a single data vector (like our document vectors) and predicts the label\n",
    "\n",
    "A nice way of presenting the performance of a classifier is to use a confusion matrix. This is a matrix where the rows show the correct label and columns the predicted label. A perfect classifier would have all samples on the main diagonal and all zeros else.\n",
    "\n",
    "Luckily ```sklearn``` already provides us with a function to plot a confusion matrix ```sklearn.metrics.plot_confusion_matrix```. This takes in a trained classifier, a matrix of samples and a vector of true labels.\n",
    "\n",
    "Your task is now to:\n",
    "\n",
    "- Train the SVM Classifier (SVC) on your training data with standard parameters\n",
    "- Plot a confusion matrix for the training set\n",
    "- Plot a confusion matrix for the test set\n",
    "- Add a title to each confusion matrix\n",
    "\n",
    "Bonus Task:\n",
    "\n",
    "Calculate the f1_score using the 'micro' average. [2 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-actor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T11:34:16.889585Z",
     "start_time": "2024-06-03T11:34:16.889572Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75db8b210b46347f71616a1da70ffa2b",
     "grade": true,
     "grade_id": "TfIdfEmbeddings_D",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classifier = SVC(kernel='poly')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-leone",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6109f01ece116528d4fa58d17e2f9344",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_E_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### One Hot Encoding E) [5 points]\n",
    "\n",
    "Discuss the performance of your classifier and reason why the performance is the way it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-knife",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baffb96f47dad39f725a4c65a0e3893e",
     "grade": true,
     "grade_id": "TfIdfEmbeddings_E",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-church",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "546f2dd805fec4b8ba82d1e02b0b7104",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_F_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TfIdf Encoding\n",
    "\n",
    "Another way of creating word embeddings is to use tfidf embeddings.\n",
    "\n",
    "For this we need a set of documents from which we create a tfidf matrix. The rows of the matrix correspond to the terms while the columns correspond to the documents (see Figure 6.8 in the chapter *Vector Semantics and Embeddings*). The embedding for a certain word is then the row corresponding to that word in the tfidf matrix.\n",
    "\n",
    "We use the following equations to calculate the tfidf values:  \n",
    "\n",
    "$\\mathrm{tfidf}(t, d) = \\mathrm{tf}(t, d) * \\mathrm{idf}(t, d)$  \n",
    "$\\mathrm{tf}(t, d) = \\mathrm{log}_{10}(\\mathrm{count}(t, d) + 1)$  \n",
    "$\\mathrm{idf}(t, d) = \\mathrm{log}_{10}\\frac{N}{\\mathrm{df}(t)}$  \n",
    "$\\mathrm{df}(t) = $ number of documents $t$ appears in\n",
    "\n",
    "*Rough outline of the algorithm to create a tfidf matrix*:\n",
    "\n",
    "1. Start by building a term document matrix $TD$. Each entry in the matrix tells us how often a term $t$ appears in the document $d$. The rows are the documents, the columns are the terms.\n",
    "2. Apply the logarithm to the $TD$ matrix to create the $TD_{log}$ matrix of logarithmic counts: $TD_{log} = \\mathrm{log}_{10}(TD + 1)$.\n",
    "3. Create a df vector $\\vec{\\mathrm{df}}$ from that matrix by counting the non zero elements in each column of the TF matrix (`np.count_nonzero`). This tells in how many documents each term appears.\n",
    "4. Create an idf vector $\\vec{\\mathrm{idf}}$ from the $\\vec{\\mathrm{df}}$ by applying the logarithm: $\\vec{\\mathrm{idf}} = \\mathrm{log}_{10}\\frac{N}{\\vec{\\mathrm{df}}}$\n",
    "5. Multiply each row of $TD_{log}$ by the idf value for that term.\n",
    "\n",
    "### Example\n",
    "\n",
    "Corpus:\n",
    "\n",
    "`document_0 = [\"i\", \"have\", \"dog\", \"i\", \"dog\", \"dog\"]`  \n",
    "`document_1 = [\"a\", \"cat\", \"a\", \"cat\"]`  \n",
    "`document_2 = [\"have\", \"cat\", \"dog\"]`\n",
    "\n",
    "Index:\n",
    "\n",
    "| **term**  | a | cat  | dog | have | i |\n",
    "|-----------|---|------|---|-----|-----|\n",
    "| **index** | 0 | 1    | 2 | 3   | 4   |\n",
    "\n",
    "Term document matrix $TD$ matrix:\n",
    "\n",
    "|      | a   | cat | dog | have | i   |\n",
    "|------|-----|-----|-----|------|-----|\n",
    "| doc_0| 0   | 0   | 3   | 1    | 2   |\n",
    "| doc_1| 2   | 2   | 0   | 0    | 0   |\n",
    "| doc_2| 0   | 1   | 1   | 1    | 0   |\n",
    "\n",
    "\n",
    "$TD_{log}$ matrix (rounded to 2 decimals):\n",
    "\n",
    "\n",
    "|      | a   | cat | dog | have | i   |\n",
    "|------|-----|-----|-----|------|-----|\n",
    "| doc_0| 0.00| 0.00| 0.60| 0.30 | 0.48|\n",
    "| doc_1| 0.48| 0.48| 0.00| 0.00 | 0.00|\n",
    "| doc_2| 0.00| 0.30| 0.30| 0.30 | 0.00|\n",
    "\n",
    "\n",
    "\n",
    "Document frequency $\\vec{\\mathrm{df}}$ :\n",
    "\n",
    "|      | a   | cat | dog | have | i   |\n",
    "|------|-----|-----|-----|------|-----|\n",
    "| doc_0| 0.00| 0.00| 0.60| 0.30 | 0.48|\n",
    "| doc_1| 0.48| 0.48| 0.00| 0.00 | 0.00|\n",
    "| doc_2| 0.00| 0.30| 0.30| 0.30 | 0.00|\n",
    "| | | | | | |\n",
    "| df   | 1   | 2   | 2   | 2    | 1   |\n",
    "\n",
    "Inverse document frequency $\\vec{\\mathrm{idf}}$:\n",
    "\n",
    "|      | a   | cat | dog | have | i   |\n",
    "|------|-----|-----|-----|------|-----|\n",
    "| df   | 1   | 2   | 2   | 2    | 1   |\n",
    "| idf  | 0.48| 0.18| 0.18| 0.18 | 0.48|\n",
    "\n",
    "\n",
    "$TFIDF$ matrix:\n",
    "\n",
    "|      | a   | cat | dog | have | i   |\n",
    "|------|-----|-----|-----|------|-----|\n",
    "| doc_0| 0.00| 0.00| 0.11| 0.05 | 0.23|\n",
    "| doc_1| 0.23| 0.08| 0.00| 0.00 | 0.00|\n",
    "| doc_2| 0.00| 0.05| 0.05| 0.05 | 0.00|\n",
    "\n",
    "\n",
    "Â© Tim Metzler, Hochschule Bonn-Rhein-Sieg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-manufacturer",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac16ad1d517b616e18fe5d157e6c97fb",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_F_Description1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TfIdf Encoding A) [30 points]\n",
    "\n",
    "Complete the class ```TfIdfModel```.\n",
    "\n",
    "This class has the following three methods:\n",
    "\n",
    "- ```build_index```: Takes a list of documents (each being a list of tokens). Then it creates the dictionary ```self.index``` which maps each type to an index. In the example above the dictionary would look like this:\n",
    "```self.index = {'a': 0, 'dog': 1, 'i': 2, 'have': 3}```. You can copy your code from the OneHotModel for this function.\n",
    "\n",
    "- ```train```: Takes a list of documents (each being a list of tokens). This should train the tfidf model such that it can return vectors for each type in the corpus\n",
    "\n",
    "- ```embed```: Take a single word and return a tfidf vector for that word. If the word is not in the index it should return ```None```\n",
    "\n",
    "\n",
    "Please complete the three methods of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-australia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T11:39:27.483460Z",
     "start_time": "2024-06-03T11:39:27.471248Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f8799e59936df7e2d1f87e77af1560c",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_F",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class TfIdfModel(EmbeddingModel):\n",
    "    \n",
    "    def build_index(self, docs: List[List[str]]) -> None:\n",
    "        '''\n",
    "        Create an index for the vocabulary from the docs\n",
    "        \n",
    "        Args:\n",
    "            docs -- A list of documents where each document is a list of tokens\n",
    "        '''\n",
    "        self.index = dict()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def train(self, docs: List[List[str]]) -> None:\n",
    "        '''\n",
    "        Train our model with a list of documents\n",
    "        \n",
    "        Args:\n",
    "            docs -- A list of documents where each document is a list of tokens\n",
    "        '''\n",
    "        \n",
    "        self.build_index(docs)\n",
    "        # Create an empty tfidf matrix\n",
    "        self.tfidf_matrix = np.zeros((len(docs), len(self.index)))\n",
    "        # Create an empty term document matrix\n",
    "        term_doc_matrix = np.zeros((len(docs), len(self.index)))\n",
    "        # Create an empty df vector\n",
    "        df_vector = np.zeros(len(self.index))\n",
    "        \n",
    "        # Fill the matrix and compute the tfidf matrix\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def embed(self, word: str) -> np.ndarray:\n",
    "        '''\n",
    "        Embed a word into our tfidf vector space\n",
    "        If the word is not in the index it will return None\n",
    "        \n",
    "        Args:\n",
    "            word      -- The word we want an embedding for\n",
    "        Returns:\n",
    "            embedding -- The tfidf embedding for the word\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def vector_size(self) -> int:\n",
    "        return self.tfidf_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-georgia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T11:47:15.671565Z",
     "start_time": "2024-06-03T11:47:15.661779Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18c0b8d7dfdc4263cb16eb53221d6bcb",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_F_Debug",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Here you can verify the working of your tfidf model\n",
    "\n",
    "# Create a tfidf model and train it on a dummy corpus\n",
    "model = TfIdfModel()\n",
    "corpus = [\n",
    "    [\"i\", \"have\", \"dog\", \"i\", \"dog\", \"dog\"],\n",
    "    [\"a\", \"cat\", \"a\", \"cat\"],\n",
    "    [\"have\", \"cat\", \"dog\"]\n",
    "]\n",
    "\n",
    "# Train the model on the corpus\n",
    "model.train(corpus)\n",
    "\n",
    "'''Show the index of the model\n",
    "Should be: \n",
    "{'a': 0, 'cat': 1, 'dog': 2, 'have': 3, 'i': 4} -> Alphabetically sorted!\n",
    "'''\n",
    "print('The index of your model is:')\n",
    "print(model.index)\n",
    "\n",
    "# Create a document embedding for the sample document\n",
    "doc = ['i', 'have', 'a', 'cool', 'dog']\n",
    "\n",
    "# This should create the embedding: ([0.09666774 0.05691117 0.02650438])\n",
    "print(f'The document embedding for {doc} is:')\n",
    "print(bagOfWords(model, doc))\n",
    "print()\n",
    "\n",
    "# Show the internal tfidf matrix for this corpus:\n",
    "'''\n",
    "Expected is:\n",
    "[[0.         0.         0.1060175  0.05300875 0.22764469]\n",
    " [0.22764469 0.08401688 0.         0.         0.        ]\n",
    " [0.         0.05300875 0.05300875 0.05300875 0.        ]]\n",
    "'''\n",
    "\n",
    "print('Tfidf matrix of your model:\\n', model.tfidf_matrix)\n",
    "\n",
    "print(model.embed(\"have\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-forward",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T14:08:15.599820Z",
     "start_time": "2024-04-29T14:08:15.576209Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9fa61a4db8ea0a6b6b545d16b7d37da",
     "grade": true,
     "grade_id": "test_TfIdfEmbeddings_F0",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "assured-communications",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18d8733b00804b786d9255615a50eb2e",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_G_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TfIdf Encoding B) [5 points]\n",
    "\n",
    "To build tfidf document vectors we can use the same approach as for the one hot document vectors.\n",
    "\n",
    "Train your TfIdfModel on the reviews from the training set.\n",
    "\n",
    "Then create the following matrices / vectors from the training and test dataset:\n",
    "\n",
    "- ```embed_train```: A 2-dimensional numpy array where the rows are the document embeddings for each document in the training set\n",
    "- ```labels_train```: A 1-dimensional numpy array where each element is the rating (stars) of the review from the training set. The rating at position 3 should correspond to the third row of the ```embed_train``` matrix.\n",
    "- ```embed_test```: A 2-dimensional numpy array where the rows are the document embeddings for each document in the test set\n",
    "- ```labels_test```: A 1-dimensional numpy array where each element is the rating (stars) of the review from the test set. The rating at position 3 should correspond to the third row of the ```embed_test``` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-clinton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T14:08:58.205554Z",
     "start_time": "2024-04-29T14:08:57.786095Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80a6daa77e5ab308b5335954ef5c7fa0",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_G",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = TfIdfModel()\n",
    "model.train([review['tokens'] for review in train])\n",
    "\n",
    "embed_train = np.array([[]])\n",
    "labels_train = np.array([])\n",
    "\n",
    "embed_test = np.array([[]])\n",
    "labels_test = np.array([])\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(embed_train.shape)  # Should print (400, 400)\n",
    "print(labels_train.shape) # Should print (400, )\n",
    "print(embed_test.shape)   # Should print (100, 400)\n",
    "print(labels_test.shape)  # Should print (100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-application",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T13:18:46.215663Z",
     "start_time": "2024-04-25T13:18:46.104182Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "133203711809065bdaf08b4aa4154632",
     "grade": true,
     "grade_id": "test_TfIdfEmbeddings_G0",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-planning",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "306c1df6cbdb9b596a2e4a7670a9015f",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_H_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TfIdf Encoding C) [5 points]\n",
    "\n",
    "Similar to task One Hot Encoding D) we want to train a classifier. \n",
    "\n",
    "- Train the SVM Classifier (SVC) on your training data with standard parameters\n",
    "- Plot a confusion matrix for the training set\n",
    "- Plot a confusion matrix for the test set\n",
    "- Add a title to each confusion matrix\n",
    "\n",
    "Bonus Task:\n",
    "\n",
    "Calculate the f1_score using the 'micro' average. [2 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-print",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T14:09:01.023056Z",
     "start_time": "2024-04-29T14:09:00.344024Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a520265c7332d0b8040b525f7d60edbc",
     "grade": true,
     "grade_id": "TfIdfEmbeddings_H",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classifier = SVC(kernel='poly')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-vision",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f13cb04415caa5f82faee9f086b07d9",
     "grade": false,
     "grade_id": "TfIdfEmbeddings_I_Description0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TfIdf Encoding D) [10 points]\n",
    "\n",
    "Discuss the performance of the TfIdf Embeddings for this task compared to the performance of the One Hot Embeddings. Which one is better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-defendant",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "041b6fb2de4d7183da1274f7f449d81a",
     "grade": true,
     "grade_id": "TfIdfEmbeddings_I",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
